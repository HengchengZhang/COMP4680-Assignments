{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>COMP4680/8650: Advanced Topics in Machine Learning</h1>\n",
    "<h2>Assignment #5: Programming Assignment</h2>\n",
    "Semester 2, 2022<br>\n",
    "</center>\n",
    "    \n",
    "**Due**: 11:55pm on Sunday 16 October, 2022.<br>\n",
    "Submit as a single Jupyter Notebook via Wattle. Make sure that your name and student ID appears in the section below. Cite all sources.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** Hengcheng Zhang\n",
    "<br>\n",
    "**Student ID:** u7096187"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This assignment gives you an opportunity to explore applications of convex optimization and actually\n",
    "solve some convex programs. We will provide you with starter Python code and comments on where you\n",
    "are expected to complete the implementation.\n",
    "\n",
    "For part of the assignment you will be using the Python package CVXPY (version 1.2), which can be\n",
    "downloaded from http://www.cvxpy.org/. Follow the installation instructions and browse through\n",
    "some of the user documentation (we will step you through most of what you need to know for solving\n",
    "the problems in this assignment).\n",
    "\n",
    "**Run all code blocks from start to end (`Restart & Run All`) and then save your Jupyter Notebook\n",
    "before submitting your assignment to ensure everything works as expected.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "\n",
    "import cvxpy as cvx\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Unconstrained Optimization (60 marks)\n",
    "\n",
    "In this question you will complete an implementation of Newton's method for solving the following unconstrained convex optimization problem,\n",
    "\n",
    "$$\n",
    "    \\text{minimize} \\quad f(x) \\triangleq - \\sum_{i=1}^{n} \\log (2 - x_i^2) - \\sum_{i=1}^{m} \\log (b_i - a_i^T x)\n",
    "$$\n",
    "\n",
    "Data for the problem is generated randomly as\n",
    "\n",
    "$$\n",
    "    A = \\begin{bmatrix} a_1^T \\\\ a_2^T \\\\ \\vdots \\\\ a_m^T \\end{bmatrix} \\in \\mathbb{R}^{m \\times n}\n",
    "    \\quad \\text{and} \\quad\n",
    "    b \\in \\mathbb{R}_{++}^{m}\n",
    "$$\n",
    "\n",
    "Since $b \\succ 0$ we know that $x = 0$ is a feasible point for initialising our optimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a problem instance\n",
    "rnd.seed(4680)\n",
    "\n",
    "m = 1000\n",
    "n = 100\n",
    "A = rnd.randn(m, n)\n",
    "b = rnd.rand(m, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Derive an expression for the gradient $g = \\nabla f(x)$\n",
    "\n",
    "$\\nabla f(x) = \\begin{bmatrix} g_1 \\\\ g_2 \\\\ \\vdots \\\\ g_m \\end{bmatrix}$, where $g_i = \\frac{2x_i}{2-x_i^2} + \\sum_{j=1}^m\\frac{a_{j,i}}{b_j - a_j^T x}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Derive an expression for the Hessian $H = \\nabla^2 f(x)$\n",
    "\n",
    "*(enter your answer here)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Complete functions `objective(x)`, `gradient(x)` and `hessian(x)` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  4,  9, 16])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([1,2,3,4])\n",
    "a = pow(a,2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(x):\n",
    "    \"\"\"Returns the value of the objective function at x.\"\"\"\n",
    "\n",
    "    r = b - A.T @ x\n",
    "    if np.any(pow(x, 2) >= 2) or r <= 0:\n",
    "        raise ValueError(\"the input x is not in domain\")\n",
    "    return -1 * np.sum(np.log(2 - pow(x, 2))) - np.sum(np.log(r))\n",
    "\n",
    "\n",
    "def gradient(x):\n",
    "    \"\"\"Returns the gradient of the objective function at x.\"\"\"\n",
    "\n",
    "    \n",
    "    ###########################################################################\n",
    "    # TODO: Implement the gradient of the objective function.                 #\n",
    "    ###########################################################################\n",
    "    pass\n",
    "\n",
    "\n",
    "def hessian(x):\n",
    "    \"\"\"Returns the Hessian of the objective function at x.\"\"\"\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the Hessian of the objective function.                  #\n",
    "    ###########################################################################\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Implement the `linesearch(...)` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linesearch(f, df, x, dx, alpha=0.3, beta=0.7):\n",
    "    \"\"\"\n",
    "    Implements backtracking line search on function f. See B&V Algorithm 9.2.\n",
    "\n",
    "    :param f: The function being optimized.\n",
    "    :param df: Gradient of the function at x.\n",
    "    :param x: Starting point for line search.\n",
    "    :param dx: Direction of line search.\n",
    "    :param alpha: Line search parameter for stopping criteria.\n",
    "    :param beta: Line search parameter for reducing step size.\n",
    "    :return: Step size t.\n",
    "    \"\"\"\n",
    "\n",
    "    t = 1.0\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the backtracking line search algorithm.                 #\n",
    "    ###########################################################################\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) Run the following code block to test your code\n",
    "\n",
    "The implementation of `gradient_descent` and `newton` are provided for you. Both use the `linesearch` function you implemented above for backtracking line search. You simply need to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, f, g, eps=1.0e-6, max_iters=200, alpha=0.3, beta=0.7):\n",
    "    \"\"\"\n",
    "    Implements gradient descent to minimize function f. See B&V Algorithm 9.3.\n",
    "\n",
    "    :param x: Starting point in domain of f.\n",
    "    :param f: The function to be optimized. Returns scalar.\n",
    "    :param g: The gradient function. Returns vector in R^n.\n",
    "    :param eps: Tolerance for stopping.\n",
    "    :param max_iters: Maximum number of iterations for stopping.\n",
    "    :param alpha: Backtracking line search parameter.\n",
    "    :param beta: Backtracking line search parameter.\n",
    "    :return: Optimization path (i.e., array of x's). The last point is the optimal point.\n",
    "    \"\"\"\n",
    "\n",
    "    path = [x.copy()]\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "        # Compute gradient\n",
    "        dx = -1.0 * g(x)\n",
    "\n",
    "        # Stopping criterion\n",
    "        print(\"...iter {}, f(x) = {}\".format(iter, f(x)))\n",
    "        if np.linalg.norm(dx) <= eps:\n",
    "            break\n",
    "\n",
    "        # Line search\n",
    "        t = linesearch(f, g(x), x, dx, alpha, beta)\n",
    "\n",
    "        # Update\n",
    "        x += t * dx\n",
    "        path.append(x.copy())\n",
    "\n",
    "    return path\n",
    "\n",
    "\n",
    "def newton(x, f, g, H, eps=1.0e-6, max_iters=200, alpha=0.3, beta=0.7):\n",
    "    \"\"\"\n",
    "    Implements Newton's method to minimize function f. See B&V Algorithm 9.5.\n",
    "\n",
    "    :param x: Starting point in domain of f.\n",
    "    :param f: The function to be optimized. Returns scalar.\n",
    "    :param g: The gradient function. Returns vector in R^n.\n",
    "    :param H: The Hessian function. Returns matrix in R^{n \\times n}.\n",
    "    :param eps: Tolerance for stopping.\n",
    "    :param max_iters: Maximum number of iterations for stopping.\n",
    "    :param alpha: Backtracking line search parameter.\n",
    "    :param beta: Backtracking line search parameter.\n",
    "    :return: Optimization path (i.e., array of x's). The last point is the optimal point.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize optimization path\n",
    "    path = [x.copy()]\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "        # Compute Newton step and decrement\n",
    "        dx = -1.0 * np.linalg.solve(H(x), g(x))\n",
    "        lmd2 = -1.0 * np.dot(g(x).T, dx)\n",
    "\n",
    "        # Stopping criterion\n",
    "        print(\"...iter {}, f(x) = {}\".format(iter, f(x)))\n",
    "        if 0.5 * lmd2 <= eps:\n",
    "            break\n",
    "\n",
    "        # Line search\n",
    "        t = linesearch(f, g(x), x, dx, alpha, beta)\n",
    "\n",
    "        # Update\n",
    "        x += t * dx\n",
    "        path.append(x.copy())\n",
    "\n",
    "    return path\n",
    "\n",
    "# --- test -----------------------------------------------------------------------------------------------\n",
    "\n",
    "# solve using gradient descent\n",
    "gd_path = gradient_descent(np.zeros((n, 1)), objective, gradient)\n",
    "\n",
    "# solve using newton's method\n",
    "nm_path = newton(np.zeros((n, 1)), objective, gradient, hessian)\n",
    "\n",
    "if objective(nm_path[-1]) < objective(gd_path[-1]):\n",
    "    x_star = nm_path[-1]\n",
    "else:\n",
    "    x_star = gd_path[-1]\n",
    "p_star = objective(x_star)\n",
    "\n",
    "# plot\n",
    "plt.figure()\n",
    "plt.semilogy(range(len(gd_path)), [objective(x) - p_star for x in gd_path], lw=2)\n",
    "plt.semilogy(range(len(nm_path)), [objective(x) - p_star for x in nm_path], lw=2)\n",
    "plt.xlabel(\"$k$\"); plt.ylabel(r\"$f(x) - p^\\star$\")\n",
    "plt.legend([\"Gradient Descent\", \"Newton's Method\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f) Implement gradient descent without line search\n",
    "\n",
    "We will implement gradient descent with a fixed step size and decaying step size schedule. This is commonly done when evaluating the function (i.e., performing line search) is expensive. However, the cost is often more iterations of the optimisation algorithm. Pseudo-code for the algorithm is\n",
    "\n",
    "---\n",
    "\n",
    "**given** a starting point $x \\in \\textbf{dom} f$, a starting step size $t > 0$, and decay rate $0 < \\gamma \\leq 1$\n",
    "\n",
    "**repeat**\n",
    "1. $\\Delta x_{nsd} := âˆ’\\nabla f(x) / \\|\\nabla f(x)\\|$.\n",
    "2. if $x + t \\Delta x_{nsd}$ feasible, then $x := x + t \\Delta x_{nsd}$.\n",
    "3. $t := \\gamma t$.\n",
    "\n",
    "**until** stopping criterion is satisfied.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_no_linesearch(x, f, g, eps=1.0e-6, max_iters=500, t=1.0e-3, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Implements gradient descent with fixed step size to minimize function f.\n",
    "\n",
    "    :param x: Starting point in domain of f.\n",
    "    :param f: The function to be optimized. Returns scalar.\n",
    "    :param g: The gradient function. Returns vector in R^n.\n",
    "    :param eps: Tolerance for stopping.\n",
    "    :param max_iters: Maximum number of iterations for stopping.\n",
    "    :param t: Initial step size parameter.\n",
    "    :param gamma: Step size decay schedule (set to 1.0 for fixed step size).\n",
    "    :return: Optimization path (i.e., array of x's). The last point is the optimal point.\n",
    "    \"\"\"\n",
    "\n",
    "    path = [x.copy()]\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: Implement gradient descent with step size schedule. You can use   #\n",
    "    # the `gradient_descent` function above as an example. Don't forget to    #\n",
    "    # ensure that x remains feasible. If x becomes infeasible then do not     #\n",
    "    # take that step.                                                         #\n",
    "    ###########################################################################    \n",
    "    \n",
    "\n",
    "    return path\n",
    "\n",
    "# --- test -----------------------------------------------------------------------------------------------\n",
    "\n",
    "# solve using gradient descent with fixed step size\n",
    "gd_fixed = gradient_descent_no_linesearch(np.zeros((n, 1)), objective, gradient, t=1.0e-3)\n",
    "\n",
    "# solve using gradient descent with decaying step size\n",
    "gd_decay = gradient_descent_no_linesearch(np.zeros((n, 1)), objective, gradient, t=5.0e-3, gamma=0.99)\n",
    "\n",
    "# plot\n",
    "plt.figure()\n",
    "plt.semilogy(range(len(gd_path)), [objective(x) - p_star for x in gd_path], lw=2)\n",
    "plt.semilogy(range(len(nm_path)), [objective(x) - p_star for x in nm_path], lw=2)\n",
    "plt.semilogy(range(len(gd_fixed)), [objective(x) - p_star for x in gd_fixed], lw=2)\n",
    "plt.semilogy(range(len(gd_decay)), [objective(x) - p_star for x in gd_decay], lw=2)\n",
    "plt.xlabel(\"$k$\"); plt.ylabel(r\"$f(x) - p^\\star$\")\n",
    "plt.legend([\"Gradient Descent\", \"Newton's Method\", \"Fixed Gradient Descent (t=1e-3)\", \"Decaying Gradient Descent (t=5e-3, gamma=0.99)\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (g) What can you say about the speed of convergence of Newton's method compared to gradient descent? What can you say about line search compared to no line search?\n",
    "\n",
    "*(enter your answer here)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear Programming (20 marks)\n",
    "\n",
    "Write a Python script using `cvx` to solve the following linear over $x \\in \\mathbb{R}^4$:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "    \\text{minimize} & 8x_1 + 6x_2 + 5x_3 + 0x_4 \\\\\n",
    "    \\text{subject to} & x_1 + x_2 + x_3 + x_4 = 1 \\\\\n",
    "    & x_1 - x_2 + x_3 - x_4 = 0 \\\\\n",
    "    & x_1, x_2, x_3, x_4 \\geq 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Include a printout of the optimal value and solution ($p^\\star$ and $x^\\star$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "c = np.array([8, 6, 5, 0])\n",
    "A = np.array([[1, 1, 1, 1], [1, -1, 1, -1]])\n",
    "b = np.array([1, 0])\n",
    "\n",
    "###########################################################################\n",
    "# TODO: Implement your CVX program here.                                  #\n",
    "###########################################################################\n",
    "\n",
    "print(\"p^\\\\star is {}\".format(p.value))\n",
    "print(\"x^\\\\star is {}\".format(x.value.transpose()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Total Variation Denoising (20 marks)\n",
    "\n",
    "In this question we investigate the problem of signal denoising.\n",
    "Consider a signal $x \\in \\mathbb{R}^n$ corrupted by noise.\n",
    "We measure the corrupted signal $x_{\\text{corr}}$ and wish to recover a good estimate $\\hat{x}$ of the original signal.\n",
    "To do this we solve the total variation denoising problem\n",
    "\n",
    "$$\n",
    "    \\text{minimize} \\quad \\|\\hat{x} - x_{\\text{corr}}\\|_2^2 + \\lambda \\|D \\hat{x}\\|_1\n",
    "$$\n",
    "\n",
    "where $D$ is the discrete derivative operator.\n",
    "Using the data supplied in `x_corr` below write a `cvx` program to solve the above optimization problem.\n",
    "\n",
    "You should experiment with your code to find a \"good\" value for $\\lambda$. Include a plot of the recovered signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_corr = np.array([[-0.0975398, -0.05312629, -0.02208009, 0.07036122, 0.05844751, -0.09111081,\n",
    "    0.25524962, 0.11391867, 0.0960413,  0.11087175, -0.07009398, 0.05289032,\n",
    "    0.04535016, 0.11058097, 0.06159582, 0.11416539, 0.14135736, 0.15779885,\n",
    "    0.11841745, -0.15573929, 0.1497802, 0.21942491, 0.26002953, 0.00406207,\n",
    "    0.01076279, 0.08560526, 0.0758106,  0.25218958, 0.17094868, 0.22549197,\n",
    "    0.14582193, 0.04450351 -0.06080554, 0.10889565, 0.00731979, 0.25383178,\n",
    "    0.21807304, 0.25086839, 0.18829612, 0.11829805, 0.13624649, 0.19156383,\n",
    "    0.13209717, 0.06982474, 0.32671382, 0.31943926, 0.17156955, 0.29698227,\n",
    "    0.20784119, 0.39387925, 1.28915111, 1.28466809, 1.17170899, 1.27276483,\n",
    "    1.30876264, 1.20624095, 1.4475541,  1.46402195, 1.20518098, 1.33903493,\n",
    "    1.46396276, 1.16194968, 1.46954723, 1.17276413, 1.40154012, 1.3344524,\n",
    "    1.26395560, 1.26554295, 1.38484967, 1.36392024, 1.47217226, 1.36321748,\n",
    "    1.57373381, 1.37651055, 1.19416729, 1.33185537, 1.27173017, 1.56374847,\n",
    "    1.31351920, 1.58509611, 1.37037572, 1.31357229, 1.25089095, 1.30896004,\n",
    "    1.16045129, 1.4191379,  1.36266027, 1.6084522,  1.62853032, 1.3224233,\n",
    "    1.37852896, 1.39204502, 1.41821484, 1.4482654,  1.40243864, 1.38977182,\n",
    "    1.37571206, 1.56182708, 1.23933557, 1.53008693, 0.33778563, 0.53671323,\n",
    "    0.39959707, 0.57810608, 0.34714623, 0.29574822, 0.52339211, 0.45073105,\n",
    "    0.40999754, 0.34623906, 0.33349269, 0.51698474, 0.60916496, 0.49911337,\n",
    "    0.45151697, 0.66435914, 0.45909372, 0.43361937, 0.4120461,  0.73994126,\n",
    "    0.42003412, 0.46914222, 0.45475553, 0.59319831, 0.44271781, 0.43542827,\n",
    "    0.54085238, 0.4293042,  0.40759905, 0.60673875, 0.57559843, 0.50406625,\n",
    "    0.42045591, 0.54069747, 0.54812155, 0.43266464, 0.3882286,  0.73634181,\n",
    "    0.48057600, 0.59402863, 0.37636053, 0.54966699, 0.48076493, 0.47702163,\n",
    "    0.38324708, 0.41276572, 0.5387545,  0.52499184, 0.60955938, 0.2569498,\n",
    "    1.59757400, 1.49974750, 1.40716928, 1.45590647, 1.41881918, 1.63509243,\n",
    "    1.56638179, 1.61757557, 1.45918297, 1.5779364,  1.51591186, 1.61326905,\n",
    "    1.49489181, 1.54304499, 1.56310244, 1.44630669, 1.50400512, 1.46626777,\n",
    "    1.53122419, 1.5964117,  1.47436147, 1.43470409, 1.65776344, 1.5155677,\n",
    "    1.45429071, 1.53389094, 1.47690731, 1.40044678, 1.46996803, 1.46289985,\n",
    "    1.39214378, 1.61761391, 1.52735972, 1.50968299, 1.55671309, 1.49293167,\n",
    "    1.53236883, 1.64304662, 1.6484901,  1.34768638, 1.34205666, 1.37332403,\n",
    "    1.5150243,  1.29612487, 1.28611079, 1.46331723, 1.25927328, 1.42103747,\n",
    "    1.46826865, 1.44057849, 0.46655008, 0.46351578, 0.40746583, 0.51540203,\n",
    "    0.49632221, 0.4754424,  0.4461718,  0.3927947,  0.47569079, 0.60111625,\n",
    "    0.46577111, 0.58634933, 0.4030563,  0.47001092, 0.33429442, 0.44149961,\n",
    "    0.41634793, 0.10497809, 0.41269741, 0.50849201, 0.25793693, 0.41901946,\n",
    "    0.47958449, 0.45448622, 0.4556222,  0.21116683, 0.31353546, 0.48157662,\n",
    "    0.51232681, 0.40093506, 0.50693641, 0.50623056, 0.55246491, 0.58091882,\n",
    "    0.09088771, 0.23097229, 0.27061655, 0.44344447, 0.25751509, 0.42937313,\n",
    "    0.43329958, 0.32736408, 0.29068132, 0.27797564, 0.28634859, 0.45557439,\n",
    "    0.4079212,  0.22503599, 0.12318214, 0.47911303, 1.3711563,  1.25869232,\n",
    "    1.42184876, 1.26588232, 1.2496218,  1.47368919, 1.45357969, 1.27337315,\n",
    "    1.38442839, 1.44858353, 1.25867526, 1.21725032, 1.31520763, 1.23095494,\n",
    "    1.27825279, 1.39025742, 1.33612252, 1.16553053, 0.91782687, 1.14797106,\n",
    "    1.2204012,  1.16296689, 1.26227372, 1.34358803, 1.10705193, 1.37268357,\n",
    "    1.38178853, 1.42554157, 1.02966171, 1.06281221, 0.91270071, 1.10881591,\n",
    "    1.03056827, 1.11251789, 1.07339657, 1.30644577, 1.12965944, 1.05443219,\n",
    "    1.12534765, 1.19720807, 1.00582764, 1.18393038, 1.25752049, 1.12823257,\n",
    "    1.26705351, 1.02434442, 1.07000497, 0.87731374, 1.03442913, 1.22304696]]).T\n",
    "n = len(x_corr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_corr, linewidth=2, color='b')\n",
    "\n",
    "###########################################################################\n",
    "# TODO: Implement your code here.                                         #\n",
    "###########################################################################\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "6b55f2c5758f94475f4f7183eca0917db64713a141a094f87423d7ec135e764d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
