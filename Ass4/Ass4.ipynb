{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hengcheng Zhang\n",
    "\n",
    "u7096187"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Conjugate functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)\n",
    "\n",
    "$f^*(y) = \\sup_{x \\in dom(f)} \\{ x^Ty - f(x) \\}$\n",
    "\n",
    "$f^*(0) = \\sup_{x \\in dom(f)} \\{-f(x) \\} \\geq -f(x) \\text{ }\\forall x \\in dom(f)$\n",
    "\n",
    "$\\therefore -f^*(0) \\leq f(x) \\text{ }\\forall x \\in dom(f)$\n",
    "\n",
    "$\\therefore -f^*(0) = \\inf_x f(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)\n",
    "\n",
    "$f^*(y) \\geq x^Ty - f(x) \\text{, } \\forall x \\in dom(f), \\forall y$\n",
    "\n",
    "We can apply the extended-value extension to get $\\tilde{f}(x) = \\begin{cases} f(x) & x \\in dom(f) \\\\ \\infty & x \\notin dom(f) \\end{cases}$\n",
    "\n",
    "$\\therefore f(x) + f^*(y) \\geq x^Ty \\text{, } \\forall x, \\forall y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)\n",
    "\n",
    "$\\begin{aligned} f^{**}(x) &= \\sup_{y \\in dom(f^*)} \\{ x^Ty - f^*(y) \\}\\\\\n",
    "&= \\sup_{y \\in dom(f^*)} \\{ x^Ty - \\sup_{z \\in dom(f)} \\{ y^Tz - f(z) \\} \\}\\\\\n",
    "&= \\sup_{y \\in dom(f^*)} \\{ x^Ty + \\inf_{z \\in dom(f)} \\{ f(z) -y^Tz \\} \\}\\\\\n",
    "&= \\sup_{y \\in dom(f^*)} \\inf_{z \\in dom(f)} \\{ x^Ty + f(z) -y^Tz \\}\\\\\n",
    "&\\leq \\inf_{z \\in dom(f)} \\sup_{y \\in dom(f^*)} \\{ x^Ty + f(z) - y^Tz \\} \\text{ weak max-min inequality}\\\\\n",
    "&= \\inf_{z \\in dom(f)} \\sup_{y \\in dom(f^*)} \\{ y^T(x - z) + f(z) \\}\\\\\n",
    "&= f(x) \\text{ text book page 281}\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d)\n",
    "\n",
    "Since for an arbitrary p-norm we have $\\| x \\|_\\infty \\leq \\| x \\|_p \\leq \\| x \\|_1$\n",
    "\n",
    "$\\begin{aligned} f^*(y) &= \\sup_{x \\in dom(f)} \\{ x^Ty - \\| x \\|_p \\}\\\\\n",
    "&\\leq \\sup_{x \\in dom(f)} \\{ x^Ty - \\| x \\|_1 \\}\\\\\n",
    "&= \\sup_{x \\in dom(f)} \\{ \\sum_i (x_iy_i - |x_i|) \\}\n",
    "\\end{aligned}$\n",
    "\n",
    "Since $x_i \\leq |x_i|$,\n",
    "\n",
    "$f^*(y) = \\begin{cases} 0 & y \\preceq 1 \\\\ \\infty & otherwise\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Norm approximation and least norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)\n",
    "\n",
    "Let $a_{[i]}$ denotes the i-th smallest element in $a$.\n",
    "\n",
    "$\\min \\| x1 - a\\|_1 = \\min \\sum_i^n| x - a_i |$\n",
    "\n",
    "This mimimum is achieved when $x = med(a) = \\begin{cases} a_{\\frac{n+1}{2}} & n \\text{ is odd}\\\\ \\frac{ a_{\\frac{n}{2}+1} + a_{\\frac{n}{2}} }{2} & n \\text{ is even} \\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)\n",
    "\n",
    "This problem is similar to the least-square problem where the optimal $x = (A^TA)^{âˆ’1}A^Tb$,\n",
    "\n",
    "$\\therefore$ the solution is $x = (1^T1)^{-1}1^Tb = \\frac{1^Tb}{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)\n",
    "\n",
    "Since $\\| a \\|_{\\infty} = \\max a_i$\n",
    "\n",
    "$\\min \\| x1 - a\\|_{\\infty} = \\min \\max | x - a_i |$,\n",
    "\n",
    "This mimimum is achieved when $x = \\frac{\\max a + \\min a}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d)\n",
    "\n",
    "$\\| x \\|_p = \\left( \\sum_i |x_i|^p \\right)^{\\frac{1}{p}} \\leq \\| x \\|_1$,\n",
    "\n",
    "If we want to minimize the largest norm of x which is its $l_1\\text{-norm}$,\n",
    "\n",
    "$\\therefore \\| x \\|_1$ is minimized when $x_i = 0$ for all $i$,\n",
    "\n",
    "Therefore the minimum is achieved when $x = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dual penalty function approximation problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathcal{L}(x,r,\\nu) = \\sum_{i=1}^m\\phi(r_i) + \\nu^T (r-Ax+b)$\n",
    "\n",
    "$\\therefore g(\\nu) = \\inf_{x,r} \\left( \\sum_{i=1}^m\\phi(r_i) + \\nu^T (r-Ax+b) \\right)$\n",
    "\n",
    "Since $x$ is unbounded below, we need to set $A^T\\nu = 0$ to find the infimum of the lagrangian over x,\n",
    "\n",
    "$\\begin{aligned} \\therefore g(\\nu) &= \\inf_r \\left( \\sum_{i=1}^m\\phi(r_i) + \\nu^Tr + \\nu^Tb) \\right)\\\\\n",
    "&= \\nu^Tb + \\inf_r \\left( \\sum_{i=1}^m\\phi(r_i) + \\nu^Tr) \\right)\\\\\n",
    "&= b^T\\nu - \\sup_r \\left( -\\sum_{i=1}^m \\left( \\phi(r_i) + \\nu_i r_i \\right) \\right)\\\\\n",
    "&= b^T\\nu - \\sum_{i=1}^m \\sup_{r_i} \\left(  (-\\nu_i) r_i -\\phi(r_i) \\right)\\\\\n",
    "&= b^T\\nu - \\sum_{i=1}^m \\phi^*(-\\nu_i)\n",
    "\\end{aligned}$\n",
    "\n",
    "$\\therefore $ The dual problem for the penalty function $\\phi$ is\n",
    "\n",
    "$\\begin{aligned} \\text{maximize } & b^T\\nu - \\sum_{i=1}^m \\phi^*(-\\nu_i) \\\\\n",
    "\\text{subject to } & A^T\\nu = 0\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)\n",
    "\n",
    "$\\phi^*(-\\nu_i) = -\\nu_i x - \\phi(x) \n",
    "= \\begin{cases} \\sup_x \\left( -\\nu_i x \\right) & |x| \\leq 1\\\\ \\sup_x \\left( -\\nu_i x - |x| + 1 \\right) & |x| > 1\\end{cases}$\n",
    "\n",
    "Since $- |x| + 1 < 0$ when $|x| > 1$, $\\left( -\\nu_i x - |x| + 1 \\right) < \\left( -\\nu_i x \\right)$,\n",
    "\n",
    "Therefore $\\phi^*(-\\nu_i) = \\sup_x \\left( -\\nu_i x \\right) = |\\nu_i|$ for $|\\nu_i| \\leq 1$\n",
    "\n",
    "$\\therefore \\phi^*(-\\nu_i) = \\begin{cases} |\\nu_i| & |\\nu_i| \\leq 1 \\\\ \\infty & \\text{otherwise} \\end{cases}$\n",
    "\n",
    "$\\therefore$ The Langrange dual optimization problem is\n",
    "\n",
    "$\\begin{aligned} \\text{maximize } & b^T\\nu - \\| \\nu \\|_1 \\\\\n",
    "\\text{subject to } & A^T\\nu = 0 \\\\\n",
    "& |\\nu| \\preceq 1 \\\\\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)\n",
    "\n",
    "$\\phi^*(-\\nu_i) = -\\nu_i x - \\phi(x) \n",
    "= \\begin{cases} \\sup_x \\left( -\\nu_i x - x^2 \\right) & |x| \\leq 1\\\\ \\sup_x \\left( -\\nu_i x - 2|x| + 1 \\right) & |x| > 1\\end{cases} \n",
    "= \\begin{cases} \\sup_x \\left( -(x + \\frac{1}{2}\\nu_i)^2 + \\frac{1}{4}\\nu_i^2 \\right) & |x| \\leq 1\\\\ \\sup_x \\left( (-\\nu_i - 2)x + 1 \\right) & x > 1 \\\\ \\sup_x \\left( (-\\nu_i + 2)x + 1 \\right) & x < -1 \\end{cases}$\n",
    "\n",
    "Therefore when $|x| > 1$ it is not upper bounded and when $|x| \\leq 1$ it has maximum $\\frac{1}{4}\\nu_i^2$ where $|\\nu_i| \\leq 2$,\n",
    "\n",
    "$\\therefore \\phi^*(-\\nu_i) = \\begin{cases} \\frac{1}{4}\\nu_i^2 & |\\nu_i| \\leq 2 \\\\ \\infty & \\text{otherwise} \\end{cases}$\n",
    "\n",
    "$\\therefore $ The Langrange dual optimization problem is\n",
    "\n",
    "$\\begin{aligned} \\text{maximize } & b^T\\nu - \\frac{1}{4}\\|\\nu\\|_2^2 \\\\\n",
    "\\text{subject to } & A^T\\nu = 0 \\\\\n",
    "& |\\nu| \\preceq 2 \\\\\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)\n",
    "\n",
    "From the previous part we can get,\n",
    "\n",
    "$\\begin{aligned} g(\\nu) &= \\inf_r \\left( \\sum_{i=1}^m\\phi(r_i) + \\nu^Tr + \\nu^Tb) \\right)\\\\\n",
    "& = \\inf_r \\begin{cases} 2^Tr + \\nu^Tr + \\nu^Tb, r \\succeq 0 \\\\ -1^Tr + \\nu^Tr +\\nu^Tb, r \\prec 0 \\end{cases}\\\\\n",
    "& = \\inf_r \\begin{cases} (2^T + \\nu^T)r + \\nu^Tb, r \\succeq 0 \\\\ (-1^T + \\nu^T)r +\\nu^Tb, r \\prec 0 \\end{cases}\\\\\n",
    "& = \\begin{cases} \\nu^Tb, -2 \\preceq \\nu \\preceq 1 \\\\ -\\infty, \\text{ otherwise} \\end{cases}\n",
    "\\end{aligned}$\n",
    "\n",
    "$\\therefore $ The Langrange dual optimization problem is\n",
    "\n",
    "$\\begin{aligned} \\text{maximize } & b^T\\nu\\\\\n",
    "\\text{subject to } & A^T\\nu = 0 \\\\\n",
    "& -2 \\preceq \\nu \\preceq 1 \\\\\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Estimation of mean and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)\n",
    "\n",
    "The maximum likelihood principle gives an optimization problem as\n",
    "\n",
    "maximize(over $\\theta$)  $\\log p_{\\theta} (y)$,\n",
    "\n",
    "Let $p_x(x), p_y(y)$ denote the density function of x and y respectively, wen can get $p_y(y) = ap_x(ay-b)$\n",
    "\n",
    "where $\\log p_{\\theta} (y) = \\sum_{i=1}^m \\log p_y (y_i) = \\log \\prod_{i=1}^m p_y (y_i) = m\\log a + \\sum_i^m \\log p_x (ay_i -b)$\n",
    "\n",
    "<!-- $\\sum_{i=1}^m \\log p_x (x_i) = \\sum_{i=1}^m \\log p_x (ay_i - b)$ -->\n",
    "\n",
    "Since p is also a log-concave funtion,\n",
    "\n",
    "$\\therefore$ the maximum-likelihood estimates of $a$ and $b$ given $y$ is\n",
    "\n",
    "$\\text{ maximize } m\\log a + \\sum_i^m \\log p_x (ay_i -b)\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)\n",
    "\n",
    "Since $p_x(x) = e^{-\\frac{1}{2}x^2}$\n",
    "\n",
    "The objective function $f(a,b) = m\\log a + \\sum_i^m \\log p_x e^{-\\frac{1}{2}(ay_i -b)^2} = m\\log a - \\frac{1}{2} \\sum_i^m (ay_i - b)^2$\n",
    "\n",
    "We first maximize over $b$,\n",
    "\n",
    "$f(a) = m\\log a - \\frac{1}{2} \\sum_i^m (ay_i - b)^2 = m\\log a - \\frac{a^2}{2} \\sum_i^m (y_i - \\frac{b}{a})^2$\n",
    "\n",
    "This function is maximized when $\\frac{b}{a} = med(y)$ (Similar to Q2.(a))\n",
    "\n",
    "$\\therefore f(a) = m\\log a - \\frac{a^2}{2} \\sum_i^m (y_i - med(y))^2\n",
    "$\n",
    "\n",
    "Then we maximize over $a$, where f(a) is a concave function,\n",
    "\n",
    "$\\begin{aligned} \\nabla_a f(a) = \\frac{m}{a} - a \\sum_i^m (y_i - med(y))^2 &= 0\\\\\n",
    "a^2 \\sum_i^m (y_i - med(y))^2 &= m\\\\\n",
    "\\therefore a = \\sqrt{ \\frac{m}{\\sum_i^m (y_i - med(y))^2} }\\\\\n",
    "\\end{aligned}$\n",
    "\n",
    "$\\therefore b = med(y) \\sqrt{ \\frac{m}{\\sum_i^m (y_i - med(y))^2} }\n",
    "$"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6b55f2c5758f94475f4f7183eca0917db64713a141a094f87423d7ec135e764d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
